## 연구 브리프: 토크나이저 선택이 LLM 성능에 미치는 영향

**TL;DR** 토크나이저 선택은 대규모 언어 모델(LLM)의 다운스트림 성능과 훈련 비용에 상당한 영향을 미칩니다. 본 연구는 다양한 토크나이저를 26억 매개변수 LLM에 적용하여  SentencePiece의 BPE 토크나이저가 단일 언어 및 다국어 모델에서 전반적으로 우수한 성능을 보임을 확인했습니다. 특히, 단일 언어 모델에서는 BPE-SP-33, 다국어 모델에서는 BPE-SP-100 토크나이저가 가장 좋은 성능을 나타냈습니다.

**문제정의** LLM 훈련에 있어 토크나이저의 최적 설정(알고리즘, 어휘 크기, 라이브러리)을 찾고, 토크나이저의 본질적 성능(fertility, parity)과 외적 성능(다운스트림 작업 성능) 간의 상관관계를 규명하는 것은 중요한 과제입니다. 특히 다국어 LLM의 효율적인 훈련을 위해 토크나이저의 영향을 분석하는 것이 필수적입니다.

**방법 한 줄 요약** 26억 매개변수 규모의 단일 언어 및 다국어 LLM 24개를 훈련하고 다양한 토크나이저를 적용하여 다운스트림 성능 및 훈련 비용을 비교 분석했습니다.

**핵심 결과**
* 다국어 토크나이저는 단일 언어(영어) 토크나이저 대비 최대 3배 큰 어휘 크기가 필요합니다.
* 영어 중심 토크나이저를 다국어 LLM 훈련에 사용하면 다운스트림 성능이 저하되고 최대 68%의 추가 훈련 비용이 발생합니다.
* SentencePiece의 BPE 토크나이저(BPE-SP-33, BPE-SP-100)가 단일 언어 및 다국어 모델에서 전반적으로 우수한 성능을 보입니다.
* 토크나이저의 fertility와 parity는 다운스트림 성능과 직접적인 상관관계를 가지지 않습니다.

**한계/위협** 본 연구는 26억 매개변수 규모의 모델에 대해서만 실험을 진행했으므로, 더 큰 규모의 LLM에 대한 일반화 가능성은 추가 연구가 필요합니다. 또한, BPE와 Unigram 토크나이저에 집중하여 다른 토크나이저(WordPiece, BBPE 등)에 대한 분석은 부족합니다.  다국어 데이터셋은 5개 유럽 언어(영어, 독일어, 프랑스어, 이탈리아어, 스페인어)에  한정되어 더 다양한 언어에 대한 연구가 필요합니다. 각 토크나이저에 대한 하이퍼파라미터 최적화는 수행되지 않았습니다.

**실무적 시사점** LLM 훈련 시 토크나이저 선택은 성능과 비용에 큰 영향을 미치므로, 목표 작업