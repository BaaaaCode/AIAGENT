- Claim: 토크나이저 선택은 LLM의 다운스트림 성능 및 훈련 비용에 상당한 영향을 미친다. 일반적인 토크나이저 평가 지표(fertility, parity)가 모델의 다운스트림 성능을 예측하는 데 항상 효과적인 것은 아니다. 다국어 토크나이저는 영어 토크나이저 대비 3배 큰 어휘 크기가 필요하다. 영어 중심 토크나이저를 다국어 LLM 훈련에 적용하면 다운스트림 성능 저하 및 최대 68%의 추가 훈련 비용이 발생한다.

- Method: 26억 매개변수 규모의 단일 언어 및 다국어 LLM 24개를 훈련하여 다양한 토크나이저 알고리즘과 매개변수를 비교 분석. 토크나이저의 본질적, 외적 성능을 단일 언어 및 다국어 환경에서 디코더 전용 모델에 초점을 맞춰 조사.

- Evidence/Numbers: 24개의 LLM(26억 매개변수), 다국어 토크나이저는 영어 대비 3배 큰 어휘 필요, 영어 중심 토크나이저 사용 시 최대 68% 훈련 비용 증가. 5개 유럽 언어 기반 다국어 토크나이저 분석.

- Limitations: 본문에서 연구의 한계에 대한 명시적인 언급은 없음. 다만, 26억 매개변수 규모의 모델에 대한 실험 결과이므로 더 큰 규모의 LLM에 대한 일반화 가능성은 추가 연구가 필요함을 추론할 수 있음.

---

- Claim: 토크나이저가 디코더 모델의 성능에 미치는 영향을 조사. 특히, 토크나이저의 본질적 성능과 외적 성능(다운스트림 작업 성능에 대한 영향) 간의 상관관계를 탐구.
- Method: BPE 및 Unigram 토크나이저를 훈련하고, 각 토크나이저에 대해 26억 개의 매개변수를 가진 디코더 전용 모델을 훈련. 데이터셋과 모델 하이퍼파라미터는 고정. 토크나이저 훈련 및 모델 훈련을 위한 전용 데이터셋 생성.
- Evidence/Numbers: 26억 개 매개변수 모델 사용.
- Limitations: 본문에 언급된 기존 연구들은 주로 인코더 모델에 초점. 디코더 모델, 특히 대규모 언어 모델(LLM)에서 토크나이저의 영향에 대한 연구는 부족. 본 연구에서는 BPE와 Unigram 토크나이저만 사용. 다른 토크나이저(WordPiece, BBPE 등)에 대한 분석은 부족. 모델 크기는 26억 개 매개변수로 고정. 다른 크기의 모델에서의 결과는 다를 수 있음.

---

- Claim: 데이터 토크나이저와 모델 훈련 데이터셋을 생성할 때, 다양한 데이터 도메인(Wikipedia, books, journals, Github, Stackoverflow, ArXiv)의 혼합 비율을 신중하게 조정했다.

- Method: 데이터 도메인의 혼합 비율 조정 방법에 대한 구체적인 설명은 제공되지 않았음.

- Evidence/Numbers:  제공되지 않았음.

- Limitations: 제공되지 않았음.

---

- Claim: 토크나이저의 다양한 설정(알고리즘, 언어 구성, 어휘 크기 등)이 모델의 다운스트림 성능에 미치는 영향을 측정하고자 함.

- Method: BPE 및 Unigram 토크나이징 알고리즘을 사용하여 다양한 크기의 단일 언어 및 다국어 토크나이저를 학습시키고, 각 토크나이저에 대해 2.6B 트랜스포머 기반 디코더 전용 모델을 학습시킴. 토크나이저의 성능을 내재적 평가(fertility, parity, vocabulary overlap)와 외재적 평가(다운스트림 작업 성능 측정)를 통해 분석하고 두 평가 접근 방식 간의 상관관계를 조사함.

- Evidence/Numbers: 700억 단어의 단일 언어(영어) 데이터셋과 700억 단어의 다국어(영어, 독일어, 프랑스어, 이탈리아어, 스페인어) 데이터셋을 사용. 총 24개의 토크나이저를 학습시키고, 각 토크나이저마다 2.6B 크기의 모델을 520억 토큰으로 학습.  평가를 위해 10,000개 문서의 held-out 데이터셋 사용. FLORES-200 병렬 코퍼스를 사용하여 parity 계산.

- Limitations: 본문에서 연구의 한계점은 명시적으로 언급되지 않음.

---

- Claim: 토크나이저의 선택이 모델의 계산 비용 및 성능에 영향을 미친다.  다국어 토크나이저는 단일 언어 토크나이저에 비해 fertility와 parity가 낮으며, vocabulary 크기가 클수록 fertility와 parity는 감소한다. 서로 다른 라이브러리에서 구현된 동일한 알고리즘(BPE) 기반 토크나이저라도 vocabulary overlap이 낮을 수 있다.

- Method: 단일/다국어 토크나이저를 학습하고 fertility, parity, vocabulary overlap을 계산하여 토크나이저의 특성을 비교 분석했다.  또한, 토크나이저별 모델 학습 시 계산 비용을 추정하고, 2.6B 크기의 디코더 전용 트랜스포머 모델을 각 토크나이저로 학습하여 다양한 zero-shot 다국어 downstream task에서 성능을 평가했다.

- Evidence/Numbers:
    * 그림 1, 2를 통해 단일/다국어 토크나이저의 fertility와 parity 비교.
    * 다국어 토크나이저는 영어 이외 문서에서 단일 언어 토크나이저보다 fertility가 훨씬 낮지만, 영어 문서에서는 약간만 낮음.
    * vocabulary 크기 증가에 따라 fertility와 parity 감소. 단일 언어 영어 토크나이저의 경우 33k vocabulary에서 fertility가 vocabulary 크기에 덜 의존적임.
    * Huggingface와 SentencePiece의 BPE 토크나이저 간 vocabulary overlap은 vocabulary 크기에 관계없이 낮음 (표 1). 영어 토크나이저: 약 0.74~0.77, 다국어 토크나이저: 약 0.61~0.62.
    * 2.6B 크기의 디코더 전용 트랜스포머 모델을 각 토크나이저로 52.6B 토큰 학습.
    * XNLI, MNLI, RTE, WNLI, CB, X-CSQA, XStoryCloze, PubMedQA 등의 다국어 downstream task에서

---

- Claim: 다국어 토크나이저의 성능을 다양한 관점(토크나이저 라이브러리, 알고리즘, 어휘 크기)에서 분석하여 전반적인 성능과 개별 작업에 대한 영향을 평가.

- Method: 단일 언어 및 다국어 토크나이저를 사용하여 다양한 자연어 처리 작업(독해, 상식 추론, 분류 등)에 대한 성능 비교. 평가는 여러 언어(영어, 독일어, 프랑스어, 스페인어, 이탈리아어)에 대해 진행. 가중 평균을 사용하여 언어별 작업 수 차이를 고려.

- Evidence/Numbers: 표 1은 단일 언어 및 다국어 토크나이저의 fertility 점수 비교, 표 2는 언어 및 작업 범주별 평가 작업 수, 표 3은 모든 작업에 대한 평균 성능, 표 4는 선택된 개별 작업에 대한 결과를 제시 (표는 본문에 직접적으로 제시되지 않음).

- Limitations: 본문은 연구 결과에 대한 분석 방법과 일부 결과에 대한 설명만 포함하고 있으며, 구체적인 수치 결과나 한계점에 대한 정보는 제공하지 않음. 따라서 본문만으로는 연구의 한계점을 파악할 수 없음.

---

- Claim: SentencePiece의 BPE 토크나이저(특히 BPE-SP-33, BPE-SP-100)가 단일 언어 및 다국어 모델에서 전반적으로 좋은 성능을 보인다. GPT-2 토크나이저는 다국어 모델에서 성능이 떨어진다. 토크나이저 라이브러리의 구현 방식 차이가 성능에 영향을 미친다.
- Method: 다양한 크기의 어휘(2.5k~5.0k)를 가진 여러 토크나이저(BPE, Unigram, GPT2)를 단일 언어(영어, 독일어) 및 다국어 모델에 적용하여 downstream task 성능을 비교 분석.
- Evidence/Numbers:
    * BPE-SP-33 토크나이저가 평균적으로 가장 좋은 성능.
    * BPE-SP-100 토크나이저가 다국어 모델에서 가장 좋은 성능.
    * ARC-Easy에서 최고 성능 토크나이저와 최저 성능 토크나이저 간 9% 성능 차이.
    * 표 3: 단일/다국어 토크나이저의 downstream task 평균 정확도.
    * 표 4: 특정 task에서 최고/최저 성능 토크나이저 결과 및 랜덤 성능.
    * 그림 3: 단어 처리에 필요한 평균 계산량(GFLOPS).
    * 표 5: BPE-SP와 BPE-HF의 성능 비교.
- Limitations: 평균 성능 지표는 task 간의 큰 성능 차이를 드러내지 못할 수 있음. 토크나이저 구현 방식 차이에 대한 자세한 분석은 부족.

---

- Claim: 토크나이저의 어휘 크기, 알고리즘, 라이브러리는 다운스트림 성능에 영향을 미치며, 단일 언어와 다국어 설정에서 최적의 어휘 크기는 다르다. 또한, 토크나이저의 어휘 크기는 계산 비용에 영향을 미치고, 다국어 토크나이저는 단일 언어 토크나이저에 비해 다국어 문서 처리 시 계산 비용을 줄일 수 있다.

- Method: 다양한 어휘 크기(33k, 50k, 100k), 토크나이저 알고리즘(BPE, Unigram), 라이브러리(Hugging Face, SentencePiece)를 사용하여 다운스트림 작업에서 성능을 측정하고, 계산 비용을 비교 분석했다. 독일어, 프랑스어, 이탈리아어, 스페인어, 영어 등 다양한 언어에 대한 실험을 진행했다.

- Evidence/Numbers: 단일 언어 영어 설정에서는 33k/50k의 작은/중간 크기 어휘가 더 나은 성능을 보였고(표 5), 다국어 설정에서는 독일어를 제외한 모든 경우에 더 큰 어휘 크기가 더 나은 다운스트림 성능을 가져왔다. 어휘 크기를 50k에서 더 큰 크기로 늘리면 모든 경우에 계산 비용이 증가한다(그림 3). 다국어 문서에 대한 계산 훈련 비용은 다국어 토크나이저가 단일 언어 영어 토크나이저보다 상당히 낮다(그림 3a). 훈련 비용은 특정 데이터셋에 대해 최대 68%까지 증가할 수 있다(독일어 문서에 대해 Multi-UNI-SP-50과 EN-UNI-SP-100 비교, 그림 3b 및 부록 표 11).

- Limitations: 본문에서 제시된 내용에 한계에 대한 명시적인 언급은 없다. 다만, 실험은 특정 언어 및 데이터셋에 대해서만

---

- Claim: 토크나이저의 fertility와 parity 점수는 모델의 다운스트림 성능과 직접적인 상관관계를 가지지 않으며, 작업별로 상관관계가 다르다. 낮은 fertility 점수는 좋은 다운스트림 성능을 위한 필요조건일 수 있지만 충분조건은 아니다. 다국어 토크나이저의 경우, 언어 간 균형 잡힌 학습을 통해 모든 언어에서 비슷하게 낮은 fertility 및 parity 점수를 얻을 수 있다.

- Method: 영어 및 5개 다국어(독일어, 프랑스어, 스페인어, 이탈리아어)에 대해 다양한 어휘 크기의 토크나이저를 학습하고, fertility 및 parity와 같은 intrinsic metric과 다운스트림 task 성능(XSQA, LAMBADA 등)과의 상관관계를 분석했다. Huggingface tokenizer 라이브러리와 SentencePiece 라이브러리를 비교하고, BPE 알고리즘을 사용했다.

- Evidence/Numbers:  높은 fertility는 훈련 중 최대 68%의 계산 비용 증가를 초래한다. 영어의 경우 33k 어휘 크기가 충분하지만, 다국어 모델은 최대 3배 큰 어휘 크기가 필요하다. SentencePiece 라이브러리가 Huggingface tokenizer 라이브러리보다 성능이 우수하다. 그림 4의 상관관계 히트맵은 작업 및 언어 전반에 걸쳐 뚜렷한 상관관계가 없음을 보여준다. 영어 이외의 작업에서는 주로 낮은 fertility와 높은 다운스트림 성능 간의 상관관계가 관찰되지만, 영어 작업에서는 무작위적인 양수 및 음수 상관관계가 나타난다.

- Limitations:  5개 언어에 대한 분석으로, 더 다양한 언어에 대한 연구가 필요하다. SAGE와 같은 다른 토크나이징 접근 방식의 영향은 고려되지 않았다. fertility와 다운스트림 성능 간의 상관관계 분석이 작업별로 세분화되어 있지만, 특정 작업에서 어떤 요인이 상

---

- Claim: 본 연구는 토크나이저 선택이 다국어 및 단일 언어 언어 모델의 성능에 유의미한 영향을 미친다는 것을 주장합니다. 특히 BPE-SP-33 토크나이저가 단일 언어 설정에서 최고의 성능을 보인다고 주장합니다.

- Method: 26개의 서로 다른 토크나이저를 사용하여 다국어 및 단일 언어 언어 모델을 훈련하고, zero-shot downstream 성능을 측정하여 토크나이저의 영향을 비교 분석했습니다.

- Evidence/Numbers: 26개 모델 훈련에 약 59,000 GPU 시간 소요. BPE-SP-33 토크나이저가 단일 언어 설정에서 최고 성능을 보임. 최대 650억 파라미터 모델에서 BPE-SP-33 토크나이저 사용됨.

- Limitations: 1) 각 토크나이저에 대한 하이퍼파라미터 최적화를 수행하지 않음. 2) 다른 random seed가 모델 성능에 미치는 영향을 조사하지 않음. 3) 더 큰 모델 크기로 결과 외삽 가능성 조사하지 않음. 4) few-shot 설정에 대한 결과 제공하지 않음 (zero-shot에 집중).

---

- Claim: 언어 모델은 few-shot 학습 능력을 가진다. 대규모 언어 모델 사전 훈련을 위한 다양한 소스 코드의 서브토큰화 옵션 조사. 자연어 yes/no 질문의 어려움 탐구. 효율적인 토큰화 없는 언어 표현 인코더 사전 훈련. 질의응답 문제 해결을 위한 AI2 추론 챌린지(ARC) 제안. Llama 훈련 데이터셋 재현을 위한 오픈소스 레시피 제공. 여러 언어 문장 표현 평가. 자연 발생 담화에서의 투사 조사. 심층 양방향 변환기의 언어 이해를 위한 사전 훈련. 문장 의역 코퍼스 자동 구성. 북유럽 언어를 위한 자기 회귀 언어 모델 개발. 새로운 데이터 압축 알고리즘 제안. 언어 모델링을 위한 다양한 텍스트의 800GB 데이터셋(The Pile) 구축. 자기 주의를 사용한 문자 수준 번역. 언어 모델링의 진전. 저자원 및 다국어 기계 번역을 위한 Flores-101 평가 벤치마크. 대규모 병렬 코퍼스 모델링(Zurich 병렬 코퍼스 컬렉션). Europarl 코퍼스 정제. 유럽 의회 디지털 코퍼스(DCEP) 구축. 컴퓨팅 최적 대규모 언어 모델 훈련에 대한 실증적 분석.

- Method: 다양한 논문에서 제시된 방법론들을 포함하며, 언어 모델 사전 훈련, 서브토큰화, 질의응답, 토큰화 없는 인코더, 데이터셋 구축, 다국어 평가, 문장 의역, 자기 회귀 모델, 데이터 압축, 문자 수준 번역, 코퍼스 구축 및 정제, 컴퓨팅 최적화 등의 방법이 사용됨.

- Evidence/Numbers: 800GB 데이터셋(The Pile), Flores-101 평가 벤치마크, Europarl 코퍼스, DCEP 등 구체적인 데이터셋 및 벤치마크가 언

---

- Claim: 본문은 다양한 자연어 처리 작업과 데이터셋, 그리고 토크나이저에 대한 연구들을 소개하고 있으므로 특정한 하나의 주장을 제시하지 않습니다. 각 논문은 저마다의 주장을 가지고 있습니다. 예를 들어 Hoffmann et al. (2022b)는 최적의 계산량으로 대규모 언어 모델을 훈련하는 방법을 제시하고, Lin et al. (2022)는 다국어 생성 언어 모델을 이용한 few-shot 학습에 대해 다룹니다.

- Method: 본문은 다양한 연구들을 소개하므로 각 연구마다 방법론이 다릅니다. Hoffmann et al. (2022b)는 계산 최적화된 대규모 언어 모델 훈련 방법을, Höfler and Piotrowski (2011)는 스위스 법률 텍스트의 언어학적 연구를 위한 코퍼스 구축 방법을 제시합니다.  Jin et al. (2019)는 생의학 연구 질의응답 데이터셋 PubMedQA를 구축했고, Kudo (2018)는 여러 subword 후보를 사용하여 신경망 번역 모델을 개선하는 subword regularization 방법을 제안합니다.

- Evidence/Numbers: 본문에서 제시되는 수치적 증거는 제한적입니다. Koehn (2005)의 Europarl은 통계적 기계 번역을 위한 병렬 코퍼스이며, Lai et al. (2017)의 RACE는 대규모 독해 이해 데이터셋입니다.  

- Limitations: 본문은 각 연구의 한계점을 명시적으로 제시하지 않습니다. 다만, 각 연구가 특정 데이터셋이나 특정 언어에 초점을 맞추고 있다는 점에서 일반화 가능성에 대한 한계가 내포되어 있을 수 있습니다. 예를 들어, Höfler and Piotrowski (2011)의 연구는 스위스 법률 텍스트에 국한됩니다. 또한 Petrov et al. (2023)는 언어 모델 토크나이저가 언어 간 불공정성을 야기할 수 있다는 점을 지적합니다.

---

- Claim: 본문은 여러 논문들을 참조하고 있지만, 특정 주장을 제시하는 텍스트가 아닙니다. 따라서 본문 자체의 주장은 없습니다. 다만, 참조된 논문들은 각자 자연어 처리 분야의 다양한 주제에 대한 주장을 담고 있습니다 (예: Winogrande 데이터셋, BLOOM 언어 모델, 기계 번역, 토크나이저 등).
- Method: 본문은 방법론을 제시하지 않습니다. 단지 다양한 연구들의 출처를 나열하고 있습니다. 각각의 참조 논문은 저마다의 방법론을 사용하고 있습니다.
- Evidence/Numbers: BLOOM 모델의 파라미터 수 (176B), 독일어 온라인 토론 데이터셋의 게시글 수 (백만 개), AAAI 2020, SIGIR 2017, ICASSP 2012 등의 학회 및 연도 정보가 언급되어 있습니다.
- Limitations: 본문 자체에는 특정 한계점이 없습니다. 각각의 참조 논문은 저마다의 한계점을 가지고 있을 것입니다. 본문은 이러한 한계점을 명시적으로 제시하지 않습니다.

---

- Claim: 본문은 여러 논문들을 소개하고 있으므로, 특정 주장을 제시하지 않습니다. 각 논문의 주장은 다음과 같습니다.
    * Byt5: 토큰화 없이 바이트 단위로 동작하는 사전 훈련 모델을 통해 토큰 기반 모델의 한계를 극복할 수 있다.
    * PAWS-X: 다국어 의역 식별을 위한 교차 언어적 적대적 데이터셋을 제시한다.
    *  Subword Vocabularies with Context: 문맥을 고려한 서브워드 어휘를 통해 성능 향상을 기대할 수 있다.
    * Megabyte: Multiscale Transformer를 사용하여 매우 긴 시퀀스(백만 바이트)를 예측할 수 있다.
    * HellaSwag: 기존 문장 완성 벤치마크의 한계를 지적하고, 새로운 벤치마크 데이터셋을 제안한다.
    * Robustness of NMT to Language Imbalance: 다국어 토크나이저 훈련에서 언어 불균형에 대한 신경망 기계 번역의 강건성을 분석한다.

- Method: 각 논문에서 사용된 방법은 다음과 같습니다.
    * Byt5: 바이트 단위 시퀀스 모델링, 사전 훈련
    * PAWS-X: 교차 언어적 적대적 학습을 위한 데이터셋 구축
    * Subword Vocabularies with Context: 문맥을 통합한 서브워드 어휘 생성
    * Megabyte: Multiscale Transformer 모델
    * HellaSwag: 새로운 벤치마크 데이터셋 및 평가 지표 제시
    * Robustness of NMT to Language Imbalance: 다국어 토크나이저 훈련 및 언어 불균형에 따른 NMT 성능 비교 분석

- Evidence/Numbers: 본문은 각 논문의 구체적인 수치/증거를 제시하지 않습니다.

- Limitations: 본문은 각 논문의 한계점을 제시하지 않습니다.

---

- Claim: 700억 단어 다국어 데이터셋과 700억 단어 영어 데이터셋을 구축했다.
- Method: Common Crawl WET Archives를 기반으로 생성된 Oscar, The Pile, RedPajama와 여러 단일 데이터셋을 포함하여 데이터셋을 구축했다. 다국어 데이터셋에는 독일어, 스페인어, 영어, 이탈리아어, 프랑스어 데이터가 포함되며, 영어 데이터셋은 다양한 출처에서 수집되었다.
- Evidence/Numbers: 다국어 데이터셋은 총 700억 단어로 구성되며, 각 언어별 단어 수는 표 6에 제시됨. 영어 데이터셋 또한 총 700억 단어로 구성되며, 출처별 단어 수는 표 7에 제시됨.
- Limitations: 본문에서 데이터셋 구축의 한계점은 명시적으로 언급되지 않았다.

---

- Claim: 저자들은 다양한 토크나이저 설정(SentencePiece, HuggingFace)과 GPT-3 아키텍처 기반의 26억 파라미터 언어 모델을 학습하여 어휘 중첩 등을 분석했다.
- Method: SentencePiece와 HuggingFace 라이브러리를 사용하여 토크나이저를 학습하고, GPT-3 아키텍처를 기반으로 26억 파라미터 언어 모델을 학습했다.  토크나이저 학습에는 다양한 어휘 크기(33k, 50k, 82k, 100k)와 Unigram, BPE 등의 알고리즘을 사용했다. 모델 학습에는 Adam 옵티마이저, 코사인 학습률 감쇠, BF16 정밀도, FlashAttention 2.0, Rotary Position Embeddings 등을 사용했다.
- Evidence/Numbers: 다국어 어휘와 영어 어휘 사이의 중첩은 24%에서 34%로 비교적 작았다. SentencePiece의 Unigram과 BPE 토크나이저는 HuggingFace의 BPE 토크나이저보다 서로 더 높은 중첩률을 보였다. 모델은 26억 개의 파라미터를 가지고 있다.
- Limitations: 본문은 토크나이저와 모델 학습에 사용된 특정 데이터셋에 대한 정보를 제공하지 않는다. 또한, 모델 성능에 대한 구체적인 평가 결과는 제시되지 않았다.  다국어 어휘와 영어 어휘의 중첩 비율이 낮은 원인에 대한 분석은 추가적인 연구가 필요하다. 라이브러리별 전처리 단계와 하이퍼파라미터의 유사성이 중첩률에 영향을 미친다는 주장은 추측에 불과하다.

---

- Claim: 토크나이저 종류에 따라 어휘 중복도와 훈련 비용이 다르다.
- Method: 다양한 토크나이저(EN-BPE-HF, EN-BPE-SP, EN-UNI-SP, MULTI-BPE-HF, MULTI-BPE-SP, MULTI-UNI-SP)를 사용하여 모델을 훈련하고 어휘 중복도와 훈련 비용(GFLOPs)을 측정 및 비교. vocab_size 32, 50, 82, 100 등 다양한 어휘 크기로 실험.
- Evidence/Numbers:
    * Figure 5는 토크나이저 간의 어휘 중복도를 보여줌.
    * Table 11은 토크나이저별 단어 처리에 대한 평균 계산 훈련 비용(GFLOPs)을 보여줌. 예: GPT-2-50(영어: 2.58 GFLOPs), MULTI-BPE-HF-33(영어: 2.46 GFLOPs).
    * 26개의 2.6B 매개변수 모델을 NVIDIA A100 GPU에서 훈련, 각 모델은 최대 2304 GPU 시간 소요, 총 훈련 비용은 약 59,000 GPU 시간.
- Limitations: 본문에서 제시된 내용 외 다른 한계점은 언급되지 않음.